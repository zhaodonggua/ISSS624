---
title: "In-class_EX4"
---

# **In-class Exercise 4: Preparing Spatial Interaction Modelling Variables**

#Getting Started
```{r}
pacman::p_load(tidyverse, sf, httr,
               tmap)

```


#Counting number of schools in each URA Planning Subzone

##Geocoding using SLA API
 the found data table will joined with the initial csv data table by using a unique identifier (i.e. POSTAL) common to both data tables. The output data table will then save as an csv file called found.

```{r}
#url<-"https://www.onemap.gov.sg/api/common/elastic/search"

#csv<-read_csv("data/aspatial/Generalinformationofschools.csv")
#postcodes<-csv$`postal_code`

#found<-data.frame()
#not_found<-data.frame()

#for(postcode in postcodes){
#  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
#  res<- GET(url,query=query)
  
#  if((content(res)$found)!=0){
#    print(found)
#    found<-rbind(found,data.frame(content(res))[4:13])
#  } else{
#    not_found = data.frame(postcode)
 # }
#}


```
 
```{r}
#merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
#write.csv(merged, file = "data/aspatial/schools.csv")
#write.csv(not_found, file = "data/aspatial/not_found.csv")


```
 
#Tidying schools data.frame
you will import schools.csv into R environment and at the same time tidying the data by selecting only the necessary fields as well as rename some fields.
```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)

```
## Converting an aspatial data into sf tibble data.frame
you will convert schools tibble data.frame data into a simple feature tibble data.frame called schools_sf by using values in latitude and longitude fields.
```{r}
schools_sf <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)

```
 
##Plotting a point simple feature layer
```{r}
mpsz <- st_read(dsn = "data/geospatial/",
                layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)


```
```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(schools_sf) +
  tm_dots() 
 
```
 
##Performing point-in-polygon count process
we will count the number of schools located inside the planning subzones.
```{r}
mpsz$`SCHOOL_COUNT`<- lengths(
  st_intersects(
    mpsz, schools_sf))

```
 
```{r}
 summary(mpsz$SCHOOL_COUNT)
 
``` 
# Data Integration and Final Touch-up

```{r}
business_sf <- st_read(dsn = "data/geospatial",
                      layer = "Business")

```
```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(business_sf) +
  tm_dots()



```

```{r}

mpsz$`BUSINESS_COUNT`<- lengths(
  st_intersects(
    mpsz, business_sf))
summary(mpsz$BUSINESS_COUNT)


```

Now, it is time for us to bring in the flow_data.rds saved after Hands-on Exercise 3.

```{r}
flow_data <- read_rds("data/rds/flow_data_tidy.rds")
flow_data


```
Notice that this is an sf tibble data.frame and the features are polylines linking the centroid of origins and destination planning subzone.
```{r}
mpsz_tidy <- mpsz %>%
  st_drop_geometry() %>%
  select(SUBZONE_C, SCHOOL_COUNT, BUSINESS_COUNT)


```

we will append SCHOOL_COUNT and BUSINESS_COUNT fields from mpsz_tidy data.frame into flow_data sf tibble data.frame by using the code chunk below.

```{r}
flow_data <- flow_data %>%
  left_join(mpsz_tidy,
            by = c("DESTIN_SZ" = "SUBZONE_C")) %>%
  rename(TRIPS = MORNING_PEAK,
         DIST = dist)


```

##Checking for variables with zero values


```{r}
summary(flow_data)

```
In view of this, code chunk below will be used to replace zero values to 0.99.

```{r}
#flow_data$SCHOOL_COUNT <- ifelse(
#  flow_data$SCHOOL_COUNT == 0,
#  0.99, flow_data$SCHOOL_COUNT)
#flow_data$BUSINESS_COUNT <- ifelse(
#  flow_data$BUSINESS_COUNT == 0,
#  0.99, flow_data$BUSINESS_COUNT)



```

```{r}
summary(flow_data)

```











